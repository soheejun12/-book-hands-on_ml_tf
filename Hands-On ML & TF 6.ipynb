{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter6. Decision Tree\n",
    "#### 분류, 회귀, 다중출력 작업이 가능한 기계학습 알고리즘 \n",
    "#### 복잡한 데이터 셋에 잘 작동하는 강력한 알고리즘 \n",
    "#### Random Forest의 기본 구성요소 \n",
    "\n",
    "\n",
    "## <목차>\n",
    "\n",
    "### 1) DT가 어떻게 학습, 시각화, 예측하는가\n",
    "### 2) CART알고리즘(Sickit-learn), 트리 정규화 및 회귀작업 \n",
    "### 3) DT의 한계점\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1\n",
    "## 1) Training and Visualizing a Decision Tree\n",
    "- iris 데이터를 Decision Tree에 학습 \n",
    "- Scikit-learn의 DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iris 데이터셋\n",
    "- 3종류의 아이리스에 대해 꽃받침의 길이, 너비와 꽃잎길이, 너비를 포함한 총 150개의 데이터 \n",
    "<img src = 'image\\ch4\\iris.png'>\n",
    "- 라벨값\n",
    "    - Setosa : 0\n",
    "    - Versicolor : 1\n",
    "    - Virginica : 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=2,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=42,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier #결정트리분류기 \n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:, 2:] #꽃잎(petal)의 길이와 넓이 \n",
    "y = iris.target #타겟값(정답값)\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(max_depth=2, random_state=42) #분류트리의 최대 깊이는 2\n",
    "tree_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DESCR': 'Iris Plants Database\\n====================\\n\\nNotes\\n-----\\nData Set Characteristics:\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20  0.76     0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThis is a copy of UCI ML iris datasets.\\nhttp://archive.ics.uci.edu/ml/datasets/Iris\\n\\nThe famous Iris database, first used by Sir R.A Fisher\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\nReferences\\n----------\\n   - Fisher,R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda,R.O., & Hart,P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...\\n',\n",
       " 'data': array([[ 5.1,  3.5,  1.4,  0.2],\n",
       "        [ 4.9,  3. ,  1.4,  0.2],\n",
       "        [ 4.7,  3.2,  1.3,  0.2],\n",
       "        [ 4.6,  3.1,  1.5,  0.2],\n",
       "        [ 5. ,  3.6,  1.4,  0.2],\n",
       "        [ 5.4,  3.9,  1.7,  0.4],\n",
       "        [ 4.6,  3.4,  1.4,  0.3],\n",
       "        [ 5. ,  3.4,  1.5,  0.2],\n",
       "        [ 4.4,  2.9,  1.4,  0.2],\n",
       "        [ 4.9,  3.1,  1.5,  0.1],\n",
       "        [ 5.4,  3.7,  1.5,  0.2],\n",
       "        [ 4.8,  3.4,  1.6,  0.2],\n",
       "        [ 4.8,  3. ,  1.4,  0.1],\n",
       "        [ 4.3,  3. ,  1.1,  0.1],\n",
       "        [ 5.8,  4. ,  1.2,  0.2],\n",
       "        [ 5.7,  4.4,  1.5,  0.4],\n",
       "        [ 5.4,  3.9,  1.3,  0.4],\n",
       "        [ 5.1,  3.5,  1.4,  0.3],\n",
       "        [ 5.7,  3.8,  1.7,  0.3],\n",
       "        [ 5.1,  3.8,  1.5,  0.3],\n",
       "        [ 5.4,  3.4,  1.7,  0.2],\n",
       "        [ 5.1,  3.7,  1.5,  0.4],\n",
       "        [ 4.6,  3.6,  1. ,  0.2],\n",
       "        [ 5.1,  3.3,  1.7,  0.5],\n",
       "        [ 4.8,  3.4,  1.9,  0.2],\n",
       "        [ 5. ,  3. ,  1.6,  0.2],\n",
       "        [ 5. ,  3.4,  1.6,  0.4],\n",
       "        [ 5.2,  3.5,  1.5,  0.2],\n",
       "        [ 5.2,  3.4,  1.4,  0.2],\n",
       "        [ 4.7,  3.2,  1.6,  0.2],\n",
       "        [ 4.8,  3.1,  1.6,  0.2],\n",
       "        [ 5.4,  3.4,  1.5,  0.4],\n",
       "        [ 5.2,  4.1,  1.5,  0.1],\n",
       "        [ 5.5,  4.2,  1.4,  0.2],\n",
       "        [ 4.9,  3.1,  1.5,  0.1],\n",
       "        [ 5. ,  3.2,  1.2,  0.2],\n",
       "        [ 5.5,  3.5,  1.3,  0.2],\n",
       "        [ 4.9,  3.1,  1.5,  0.1],\n",
       "        [ 4.4,  3. ,  1.3,  0.2],\n",
       "        [ 5.1,  3.4,  1.5,  0.2],\n",
       "        [ 5. ,  3.5,  1.3,  0.3],\n",
       "        [ 4.5,  2.3,  1.3,  0.3],\n",
       "        [ 4.4,  3.2,  1.3,  0.2],\n",
       "        [ 5. ,  3.5,  1.6,  0.6],\n",
       "        [ 5.1,  3.8,  1.9,  0.4],\n",
       "        [ 4.8,  3. ,  1.4,  0.3],\n",
       "        [ 5.1,  3.8,  1.6,  0.2],\n",
       "        [ 4.6,  3.2,  1.4,  0.2],\n",
       "        [ 5.3,  3.7,  1.5,  0.2],\n",
       "        [ 5. ,  3.3,  1.4,  0.2],\n",
       "        [ 7. ,  3.2,  4.7,  1.4],\n",
       "        [ 6.4,  3.2,  4.5,  1.5],\n",
       "        [ 6.9,  3.1,  4.9,  1.5],\n",
       "        [ 5.5,  2.3,  4. ,  1.3],\n",
       "        [ 6.5,  2.8,  4.6,  1.5],\n",
       "        [ 5.7,  2.8,  4.5,  1.3],\n",
       "        [ 6.3,  3.3,  4.7,  1.6],\n",
       "        [ 4.9,  2.4,  3.3,  1. ],\n",
       "        [ 6.6,  2.9,  4.6,  1.3],\n",
       "        [ 5.2,  2.7,  3.9,  1.4],\n",
       "        [ 5. ,  2. ,  3.5,  1. ],\n",
       "        [ 5.9,  3. ,  4.2,  1.5],\n",
       "        [ 6. ,  2.2,  4. ,  1. ],\n",
       "        [ 6.1,  2.9,  4.7,  1.4],\n",
       "        [ 5.6,  2.9,  3.6,  1.3],\n",
       "        [ 6.7,  3.1,  4.4,  1.4],\n",
       "        [ 5.6,  3. ,  4.5,  1.5],\n",
       "        [ 5.8,  2.7,  4.1,  1. ],\n",
       "        [ 6.2,  2.2,  4.5,  1.5],\n",
       "        [ 5.6,  2.5,  3.9,  1.1],\n",
       "        [ 5.9,  3.2,  4.8,  1.8],\n",
       "        [ 6.1,  2.8,  4. ,  1.3],\n",
       "        [ 6.3,  2.5,  4.9,  1.5],\n",
       "        [ 6.1,  2.8,  4.7,  1.2],\n",
       "        [ 6.4,  2.9,  4.3,  1.3],\n",
       "        [ 6.6,  3. ,  4.4,  1.4],\n",
       "        [ 6.8,  2.8,  4.8,  1.4],\n",
       "        [ 6.7,  3. ,  5. ,  1.7],\n",
       "        [ 6. ,  2.9,  4.5,  1.5],\n",
       "        [ 5.7,  2.6,  3.5,  1. ],\n",
       "        [ 5.5,  2.4,  3.8,  1.1],\n",
       "        [ 5.5,  2.4,  3.7,  1. ],\n",
       "        [ 5.8,  2.7,  3.9,  1.2],\n",
       "        [ 6. ,  2.7,  5.1,  1.6],\n",
       "        [ 5.4,  3. ,  4.5,  1.5],\n",
       "        [ 6. ,  3.4,  4.5,  1.6],\n",
       "        [ 6.7,  3.1,  4.7,  1.5],\n",
       "        [ 6.3,  2.3,  4.4,  1.3],\n",
       "        [ 5.6,  3. ,  4.1,  1.3],\n",
       "        [ 5.5,  2.5,  4. ,  1.3],\n",
       "        [ 5.5,  2.6,  4.4,  1.2],\n",
       "        [ 6.1,  3. ,  4.6,  1.4],\n",
       "        [ 5.8,  2.6,  4. ,  1.2],\n",
       "        [ 5. ,  2.3,  3.3,  1. ],\n",
       "        [ 5.6,  2.7,  4.2,  1.3],\n",
       "        [ 5.7,  3. ,  4.2,  1.2],\n",
       "        [ 5.7,  2.9,  4.2,  1.3],\n",
       "        [ 6.2,  2.9,  4.3,  1.3],\n",
       "        [ 5.1,  2.5,  3. ,  1.1],\n",
       "        [ 5.7,  2.8,  4.1,  1.3],\n",
       "        [ 6.3,  3.3,  6. ,  2.5],\n",
       "        [ 5.8,  2.7,  5.1,  1.9],\n",
       "        [ 7.1,  3. ,  5.9,  2.1],\n",
       "        [ 6.3,  2.9,  5.6,  1.8],\n",
       "        [ 6.5,  3. ,  5.8,  2.2],\n",
       "        [ 7.6,  3. ,  6.6,  2.1],\n",
       "        [ 4.9,  2.5,  4.5,  1.7],\n",
       "        [ 7.3,  2.9,  6.3,  1.8],\n",
       "        [ 6.7,  2.5,  5.8,  1.8],\n",
       "        [ 7.2,  3.6,  6.1,  2.5],\n",
       "        [ 6.5,  3.2,  5.1,  2. ],\n",
       "        [ 6.4,  2.7,  5.3,  1.9],\n",
       "        [ 6.8,  3. ,  5.5,  2.1],\n",
       "        [ 5.7,  2.5,  5. ,  2. ],\n",
       "        [ 5.8,  2.8,  5.1,  2.4],\n",
       "        [ 6.4,  3.2,  5.3,  2.3],\n",
       "        [ 6.5,  3. ,  5.5,  1.8],\n",
       "        [ 7.7,  3.8,  6.7,  2.2],\n",
       "        [ 7.7,  2.6,  6.9,  2.3],\n",
       "        [ 6. ,  2.2,  5. ,  1.5],\n",
       "        [ 6.9,  3.2,  5.7,  2.3],\n",
       "        [ 5.6,  2.8,  4.9,  2. ],\n",
       "        [ 7.7,  2.8,  6.7,  2. ],\n",
       "        [ 6.3,  2.7,  4.9,  1.8],\n",
       "        [ 6.7,  3.3,  5.7,  2.1],\n",
       "        [ 7.2,  3.2,  6. ,  1.8],\n",
       "        [ 6.2,  2.8,  4.8,  1.8],\n",
       "        [ 6.1,  3. ,  4.9,  1.8],\n",
       "        [ 6.4,  2.8,  5.6,  2.1],\n",
       "        [ 7.2,  3. ,  5.8,  1.6],\n",
       "        [ 7.4,  2.8,  6.1,  1.9],\n",
       "        [ 7.9,  3.8,  6.4,  2. ],\n",
       "        [ 6.4,  2.8,  5.6,  2.2],\n",
       "        [ 6.3,  2.8,  5.1,  1.5],\n",
       "        [ 6.1,  2.6,  5.6,  1.4],\n",
       "        [ 7.7,  3. ,  6.1,  2.3],\n",
       "        [ 6.3,  3.4,  5.6,  2.4],\n",
       "        [ 6.4,  3.1,  5.5,  1.8],\n",
       "        [ 6. ,  3. ,  4.8,  1.8],\n",
       "        [ 6.9,  3.1,  5.4,  2.1],\n",
       "        [ 6.7,  3.1,  5.6,  2.4],\n",
       "        [ 6.9,  3.1,  5.1,  2.3],\n",
       "        [ 5.8,  2.7,  5.1,  1.9],\n",
       "        [ 6.8,  3.2,  5.9,  2.3],\n",
       "        [ 6.7,  3.3,  5.7,  2.5],\n",
       "        [ 6.7,  3. ,  5.2,  2.3],\n",
       "        [ 6.3,  2.5,  5. ,  1.9],\n",
       "        [ 6.5,  3. ,  5.2,  2. ],\n",
       "        [ 6.2,  3.4,  5.4,  2.3],\n",
       "        [ 5.9,  3. ,  5.1,  1.8]]),\n",
       " 'feature_names': ['sepal length (cm)',\n",
       "  'sepal width (cm)',\n",
       "  'petal length (cm)',\n",
       "  'petal width (cm)'],\n",
       " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n",
       " 'target_names': array(['setosa', 'versicolor', 'virginica'],\n",
       "       dtype='<U10')}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vals = np.array([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 시각화 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    -  ※graphviz 모듈 \n",
    "        - 사이트에서 graphviz 다운로드 및 설치 \n",
    "        - 생성되는 bin 폴더의 path를 환경변수에 추가 \n",
    "        - cmd에서 .dot 파일이 존재하는 경로까지 접근 후 \n",
    "        - 책의 dot 명령어 실행 \n",
    "        - .dot 파일이 존재하는 경로에 이미지 파일이 생성됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#시각화 모듈 \n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "export_graphviz(\n",
    "        tree_clf,\n",
    "        out_file=(\"iris_tree.png\"),\n",
    "        feature_names=iris.feature_names[2:],\n",
    "        class_names=iris.target_names,\n",
    "        rounded=True, #노드 모양(둥근 사각형), 폰트 설정\n",
    "        filled=True #분류에서 중요한 노드(클래스)를 색칠해서 표현 \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'iris_tree.dot'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Making Prediction\n",
    "#### 위의 트리가 어떻게 예측되는지 과정 \n",
    "#### 아이리스 꽃을 발견했고 그것을 분류한다 가정 \n",
    "### - 깊이 0 \n",
    "<img src = 'image\\ch6\\iris_tree.png'>\n",
    "#### - 탐색은 루트노드에서 시작 \n",
    "- 꽃잎의 길이가 2.45보다 작거나 같은가\n",
    "    - 깊이 1의 자식노드로 이동 : 맞으면 왼쪽, 아니면 오른쪽 노드로 이동 \n",
    "   \n",
    "    \n",
    "### - 깊이 1 \n",
    "<img src = 'iris_tree.png'>\n",
    "#### - 왼쪽\n",
    "- 꽃잎길이가 2.45보다 작거나 같음 \n",
    "- 예상클래스 : Setosa\n",
    "- 리프노드 (자식이 없음) : 더이상 질문하지 않음  \n",
    "\n",
    "#### - 오른쪽\n",
    "- 꽃잎길이가 2.45보다 큼 \n",
    "- 추가적인 질문을 함 (리프노드가 아님) \n",
    "     - 꽃잎의 너비가 1.75보다 작거나 같은가 \n",
    "         - 깊이 2의 자식노드로 이동 \n",
    "         \n",
    "### - 깊이 2 \n",
    "<img src = 'iris_tree.png'>\n",
    "#### - 왼쪽 \n",
    "- 꽃잎의 길이가 2.45보다 크고 꽃잎의 너비가 1.75보다 작거나 같음\n",
    "- 예상클래스 : Versicolor\n",
    "- 리프노드\n",
    "\n",
    "#### - 오른쪽 \n",
    "- 꽃잎의 길이가 2.45보다 크고 꽃잎의 너비가 1.75보다 큼 \n",
    "- 예상클래스 : Virginica\n",
    "- 리프노드 \n",
    "\n",
    "#### 간단하게 예측 가능 \n",
    " \n",
    "\n",
    "## ※DT의 특징 \n",
    "#### - 특별히 많은 데이터 준비가 요구되지 않음 \n",
    "#### - feature scaling이나 centering 등의 작업도 필요하지 않음 \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - 노드의 속성 \n",
    "<img src = 'image\\ch6\\dt2.png'>\n",
    "### Samples\n",
    "#### - 해당 노드에 적용된 학습데이터의 수 \n",
    "\n",
    "### Value\n",
    "#### - 해당 노드에서 각 클래스의 학습데이터 수 \n",
    "    - 54개의 샘플 중 클래스1은 40개, 클래스2는 5개\n",
    "\n",
    "### Gini\n",
    "#### - 적용되는 학습데이터의 불순도 \n",
    "    - 만약 모든 학습데이터가 같은 클래스에 속하면 gini = 0\n",
    "        - Setosa 노드\n",
    "#### - Gini 불순도 계산 공식 (i번째 노드의 gini 점수 계산) \n",
    "<img src = 'image\\ch6\\gini.png'> \n",
    "    - P(i,k) : i번째 노드에서 학습데이터 중  클래스k에 속하는 데이터의 비율\n",
    "\n",
    "## ※Scikit- learn \n",
    "#### - Scikit-learn은 이진트리만 생성하는 CART 알고리즘 사용 \n",
    " - ID3과 같은 알고리즘을 사용하면 세개 이상의 자식을 가지는 의사결정트리 생성 가능 \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## - DT의 결정경계 \n",
    "<img src = 'iris_tree.png'>\n",
    "<img src = 'image\\ch6\\dtb.png'>\n",
    "### 굵은 선은 루트노드의 경계 \n",
    "#### -  왼쪽은 pure한 상태라 더이상 분할하지 않음 \n",
    "#### - 오른족은 impure하기 때문에 다시 분할 \n",
    "\n",
    "### 굵은 점선은 깊이 1의 오른쪽 노드의 경계 \n",
    "\n",
    "### 깊이가 늘어나면 결정경계도 늘어남 (얇은 점선) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ※ 모델 해석 : White Box VS Black Box \n",
    "### - White Box 모델 \n",
    "#### 예측을 이해, 해석하기 쉬운 모델 \n",
    "#### 예) DT \n",
    "\n",
    "### - Black Box 모델 \n",
    "#### 예측에 대한 이해, 해석하기 복잡하고 어려움 \n",
    "#### 예)  RandomForest, Nueral Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Estimating Class Probabilities \n",
    "- 데이터가 특정 클래스에 속할 확률 측정 가능 \n",
    "\n",
    "#### 1. 해당 데이터의 리프노드를 찾기 위해 탐색 \n",
    "#### 2. 해당 노드에서 클래스k의 학습데이터 비율 반환 \n",
    "\n",
    "#### 어떤 클래스에 속하는지 예측시에는 가장 높은 확률을 가진 클래스 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.90740741,  0.09259259]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#꽃잎의 길이가 5, 너비가 1.5인 꽃은 어느 클래스에 속할까\n",
    "tree_clf.predict_proba([[5, 1.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf.predict([[5, 1.5]])\n",
    "#1번째 클래스의 확률이 가장 높기 때문 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2\n",
    "## 1) CART 알고리즘 \n",
    "#### Scikit-learn은 CART(Classification And Regression Tree)알고리즘을 사용해 DT 학습 \n",
    "\n",
    "### - CART 아이디어 \n",
    "####  1. 하나의 특징(t)과 임계값(tk)를 사용해 학습데이터를 두개의 subset으로 분할 \n",
    "- t와 tk 선택 방법 \n",
    "    - 가장 순수한 부분집한 쌍 (t, tk)를 찾음 \n",
    "    - 알고리즘이 최소화하려는 비용함수 \n",
    "    <img src = 'image\\ch6\\cart.png'>\n",
    "        - m : 데이터 개수 \n",
    "        - 왼쪽 노드의 불순성과 오른쪽 노드의 불순성의 합이 최대한 최소가 되도록 \n",
    "        \n",
    "####  2. subset 분할이 성공적이면 동일한 로직을 사용해 하위 집합을 계속 분할\n",
    "- 최대깊이 도달, 더이상 불순성을 줄일 분할이 없을때 까지 진행 \n",
    "- 추가적으로 중지할 수 있는 하이퍼파리미터 존재 \n",
    "\n",
    "\n",
    "## ※주의 \n",
    "### CART 알고리즘은 탐욕(greedy) 알고리즘\n",
    "#### - 최고수준(현재단계)에서 최적의 분할을 탐색하는 과정을 반복 \n",
    "#### - 이후의 단계에서의 불순성 정도를 신경쓰지 않음 \n",
    "#### - 그러므로, CART 알고리즘은 합리적인 해결책은 제공해도 최적의 해결책이라 보장하기는 어려움 \n",
    "\n",
    "#### - 최적의 트리를 찾는 것\n",
    "- NP-Complete문제로 알려져있음 \n",
    "- O(exp(m))의 시간복잡도를 가져 적은양의 학습데이터에서도 문제를 다루기 어려움 \n",
    "\n",
    "### - 계산 복잡도 \n",
    "#### 트리 탐색시 대략 O(log2(m))개의 노드를 지나야함 \n",
    "- 예측을 위해선 DT는 루트에서 리프로 이동해야함 \n",
    "- DT는 일반적으로 어느정도의 균형을 이룸 \n",
    "\n",
    "#### 특징의 개수에 영향을 받지 않음 \n",
    "- 각 노드는 하나의 특징만을 사용\n",
    "- 특징의 개수에 상관없이 전체 예측 복잡도는 O(log2(m))\n",
    "\n",
    "#### 하지만 학습알고리즘은 데이터의 크기에 영향을 받음 \n",
    "- 학습알고리즘은 각 노드의 모든 샘플들에서 모든 특징을 비교함 \n",
    "- 학습 복잡도는 O(n × m log(m))\n",
    "- Scikit-learn에서 작은 학습데이터는 presorting으로 속도를 높일 수 있지만, 대규모 데이터에서는 속도가 꽤 느려짐 \n",
    "\n",
    "### - Gini Impurity or Entropy?\n",
    "#### 기본적으로 gini 불순도가 사용되지만 entropy불순도도 사용가능 \n",
    "- 하이퍼파라미터 (criterion = entropy)로 설정 \n",
    "\n",
    "#### - Entropy \n",
    "- 기계학습에서 불순도 측정시 자주 사용 \n",
    "- 데이터의 불순도가 낮을수록 0에 가까움 (데이터에 한가지 클래스만 존재하면 0)\n",
    "- entropy 공식 \n",
    "<img src = 'image\\ch6\\entropy.png'>\n",
    "    - p(i,k) : 노드 i의 데이터가 클래스k일 확률 \n",
    "\n",
    "#### - 언제 무엇을 사용?\n",
    "- 사실 gini와 entropy에 큰 차이는 없음 \n",
    "- 결국은 비슷한 트리로 이어짐 \n",
    "- gini가 계산속도가 조금 더 빨라 DecisionTreeClassifier의 default로 설정되어있음\n",
    "- 차이점 \n",
    "    - gini : 트리의 분기에서 가장 빈번한 클래스를 고립시키는 경향 존재 \n",
    "    - entropy : gini보다 좀 더 균형잡힌 트리 생성 \n",
    "    \n",
    "## 2) Regularization Hyperparameters\n",
    "#### DT는 학습데이터의 구조에 대한 가정을 크게 고려하지 않음  \n",
    "#### 제한을 주지 않으면 데이터 자체에 트리가 적응하여 overfitting 될 가능성 높음 \n",
    "#### Nonparametic Model \n",
    "- 예) DT\n",
    "- 매개변수의 값이 학습 이전에 결정되지 않음 \n",
    "\n",
    "#### Parametic Model \n",
    "- 예) 선형모델 \n",
    "- 미리 결정된 값의 매개변수들을 가짐 \n",
    "- 자유도 제약, overfitting 가능성 낮춰줌 (underfitting 가능성은 높아질 수 있음) \n",
    "\n",
    "#### overfitting 방지를 위해서는 DT의 자유도 제한 필요 \n",
    "- 정규화 작업 필요 \n",
    "\n",
    "### Scikit-learn DecisionTreeClassifier클래스의 DT 제한 하이퍼파라미터 \n",
    "#### - max_depth : 트리의 최대깊이 지정 \n",
    "#### - min_sample_split : 노드가 분할되기 전에 있어야할 샘플의 최소개수 지정\n",
    "#### - min_samples_leaf : 리프노드에 있어야할 샘플의 최소개수 지정 \n",
    "#### - min_weight_fractoin_leaf :  min_samples_leaf와 같지만 가중치가 부여된 데이터의 총 개수의 부분으로 표현 \n",
    "#### - max_leaf_nodes : 리프노드 최대개수 지정 \n",
    "#### - max_features : 각 노드에서 분할시 사용되는 최대 특징개수 지정 \n",
    "\n",
    "\n",
    "\n",
    "## ※\n",
    "- 다른 학습 알고리즘은 먼저 제약 없이 한 번 학습 후 필요없다고 생각되는 노드를 제거함 \n",
    "    - 필요하지 않은 노드  : 더이상 불순성 향상이 크게 의미가 없는 리프노드 \n",
    "    - 모든 리프노드를 고려하는 것은 불필요하기 때문에 제거 \n",
    "    \n",
    "- X^2 테스트와 같은 표준 통계 테스트는 개선이 순전히 우연의 결과라는 것을 증명하기 위해 사용됨 \n",
    "    - 귀무가설 : 설정한 가설이 진실할 확률이 극히 적어 처음부터 버릴 것으로 예상되는 가설 \n",
    "    - 대립가설 : 실제로 채택될 것을 희망하는 가설 (귀무가설과 반대)\n",
    "        - 귀무가설이 기각되면 대립가설이 채택\n",
    "        \n",
    "- p-value가 주어진 임계값(보통 5%)보다 높으면, 노드는 불필요한 것으로 간주되어 제거됨 \n",
    "    - p-value가 임계값 보다 크면 \n",
    "        - 오차가 커 귀무가설이 기각되지 않음 \n",
    "    - p-value가 임계값 보다 작으면 \n",
    "        - 귀무가설이 기각됨\n",
    "            -  귀무가설인 '개선이 순전히 우연의 결과'를 기각할 수 없음 \n",
    "            -  즉, 개선은 순전히 우연의 결과다 \n",
    "            - 그러므로 해당 노드의 향상은 큰 의미가 없다 \n",
    "            - 결론적으로 제거해도 된다 \n",
    "            \n",
    "- 이러한 제거(가지치기)는 모든 불필요한 노드들이 제거될때 까지 진행됨 \n",
    "\n",
    "\n",
    "### - 기본값 사용 vs 하이퍼파라미터 정규화 \n",
    "<img src = 'image\\ch6\\r1.png'>\n",
    "- 왼쪽, overfitting 됨 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Regression \n",
    "### - Scikit-learn의 DecisionTreeRegression 클래스 사용 \n",
    "- 최대깊이가 2인 DT\n",
    "- 노이즈를 포함한 2차데이터 학습 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=2, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=42, splitter='best')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#노이즈를 포함하는 2차데이터 생성 \n",
    "np.random.seed(42)\n",
    "m = 200\n",
    "X = np.random.rand(m, 1)\n",
    "y = 4 * (X - 0.5) ** 2\n",
    "y = y + np.random.randn(m, 1) / 10\n",
    "\n",
    "#회귀트리 \n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "tree_reg = DecisionTreeRegressor(max_depth=2, random_state=42) #최대깊이는 2\n",
    "tree_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#시각화 \n",
    "export_graphviz(\n",
    "        tree_reg,\n",
    "        out_file=(\"regression_tree.dot\"),\n",
    "        rounded=True,\n",
    "        filled=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'regression_tree.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - 회귀트리 \n",
    "- 데이터의 클래스를 반환하는 분류트리가 아니라 값을 예측함 \n",
    "- 예측값은 해당 노드의 데이터들의 평균 \n",
    "- MSE를 비용함수로 사용 \n",
    "\n",
    "#### - 왜 예측값이 데이터들의 평균인가 \n",
    "<img src = 'image\\ch6\\regression.png'>\n",
    "- 왼쪽, 위의 회귀트리 \n",
    "- 오른쪽, 트리의 최대깊이를 3으로 지정 \n",
    "- 빨간선이 예측값 \n",
    "- 알고리즘은 각 영역에서의 데이터들이 최대한 예측값과 가까이 위치하도록 분할함 \n",
    "    - 데이터들이 모두 가장 가깝게 위치할 수 있는 곳 -> 평균 \n",
    "    \n",
    "#### - CART 알고리즘 \n",
    "- 분류트리에서는 불순성을 최소화 \n",
    "- 회귀트리에서는 MSE를 최소화 \n",
    "<img src = 'image\\ch6\\cart2.png'>\n",
    "    - m : 데이터의 개수 \n",
    "    \n",
    "#### - 정규화 \n",
    "<img src = 'image\\ch6\\regression2.png'>\n",
    "- 왼쪽, 하이퍼파라미터를 정규화하지 않은 회귀트리 \n",
    "- 오른쪽, 정규화한 회귀트리 (리프노드의 최소 샘플수 10으로 지정)\n",
    "- 회귀트리도 default 하이퍼파라미터를 사용하면 overfitting되기 쉬움 \n",
    "- 정규화를 이용하면 훨씬 합리적인 모델이 생성될 수 있음 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3\n",
    "##  DT의 한계\n",
    "### 1) DT의 결정경계는 항상 축에 수직적으로 직교함 \n",
    "#### - 데이터의 회전에 민감함 \n",
    "<img src = 'image\\ch6\\i1.png'>\n",
    "- 단순한 선형으로 분리된 데이터\n",
    "- 왼쪽, 쉽게 분할 가능 \n",
    "- 오른쪽, 데이터가 45도 회전함 \n",
    "    - 불필요하게 굉장히 복잡한 결정경계 생성 \n",
    "    - PCA를 사용해 해결 가능 \n",
    "    \n",
    "### 2) 데이터의 작은 변화에도 매우 민감 \n",
    "#### - 하나의 데이터가 제거되어도 전혀 다른 모양의 트리가 생성될 수 있음\n",
    "- 데이터(versicolor) 삭제 전 \n",
    "<img src = 'image\\ch6\\dtb.png'>\n",
    "- 데이터(versicolor) 삭제 후 (굵은 점선 사이에 존재하는 데이터 삭제) \n",
    "<img src = 'image\\ch6\\i2.png'>\n",
    "\n",
    "#### - 실제로 scikit-learn에 의해 사용되는 훈련알고리즘이 확률적임 \n",
    "- 각 노드에서 특징을 임의로 선택 \n",
    "- 그러므로 같은 데이터에 대해서도 매우 다른 모델 생성이 가능함 \n",
    "- random_state 하리퍼파라미터로 제한 가능 \n",
    "\n",
    "#### 다음 장의 RandomForest는 많은 트리의 예측의 평균을 사용하기 떄문에 불안전성 처리 가능 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
